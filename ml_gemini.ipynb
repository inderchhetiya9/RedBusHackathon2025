{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "baeb4be6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded successfully.\n",
      "\n",
      "--- Train Data Info ---\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 67200 entries, 0 to 67199\n",
      "Data columns (total 4 columns):\n",
      " #   Column           Non-Null Count  Dtype  \n",
      "---  ------           --------------  -----  \n",
      " 0   doj              67200 non-null  object \n",
      " 1   srcid            67200 non-null  int64  \n",
      " 2   destid           67200 non-null  int64  \n",
      " 3   final_seatcount  67200 non-null  float64\n",
      "dtypes: float64(1), int64(2), object(1)\n",
      "memory usage: 2.1+ MB\n",
      "\n",
      "--- Train Data Head ---\n",
      "          doj  srcid  destid  final_seatcount\n",
      "0  2023-03-01     45      46           2838.0\n",
      "1  2023-03-01     46      45           2298.0\n",
      "2  2023-03-01     45      47           2720.0\n",
      "3  2023-03-01     47      45           2580.0\n",
      "4  2023-03-01     46       9           4185.0\n",
      "\n",
      "--- Test Data Info ---\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5900 entries, 0 to 5899\n",
      "Data columns (total 4 columns):\n",
      " #   Column     Non-Null Count  Dtype \n",
      "---  ------     --------------  ----- \n",
      " 0   route_key  5900 non-null   object\n",
      " 1   doj        5900 non-null   object\n",
      " 2   srcid      5900 non-null   int64 \n",
      " 3   destid     5900 non-null   int64 \n",
      "dtypes: int64(2), object(2)\n",
      "memory usage: 184.5+ KB\n",
      "\n",
      "--- Test Data Head ---\n",
      "          route_key         doj  srcid  destid\n",
      "0  2025-02-11_46_45  2025-02-11     46      45\n",
      "1  2025-01-20_17_23  2025-01-20     17      23\n",
      "2  2025-01-08_02_14  2025-01-08      2      14\n",
      "3  2025-01-08_08_47  2025-01-08      8      47\n",
      "4  2025-01-08_09_46  2025-01-08      9      46\n",
      "\n",
      "--- Transactions Data Info ---\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2266100 entries, 0 to 2266099\n",
      "Data columns (total 11 columns):\n",
      " #   Column              Dtype  \n",
      "---  ------              -----  \n",
      " 0   doj                 object \n",
      " 1   doi                 object \n",
      " 2   srcid               int64  \n",
      " 3   destid              int64  \n",
      " 4   srcid_region        object \n",
      " 5   destid_region       object \n",
      " 6   srcid_tier          object \n",
      " 7   destid_tier         object \n",
      " 8   cumsum_seatcount    float64\n",
      " 9   cumsum_searchcount  float64\n",
      " 10  dbd                 int64  \n",
      "dtypes: float64(2), int64(3), object(6)\n",
      "memory usage: 190.2+ MB\n",
      "\n",
      "--- Transactions Data Head ---\n",
      "          doj         doi  srcid  destid    srcid_region   destid_region  \\\n",
      "0  2023-03-01  2023-01-30     45      46       Karnataka      Tamil Nadu   \n",
      "1  2023-03-01  2023-01-30     46      45      Tamil Nadu       Karnataka   \n",
      "2  2023-03-01  2023-01-30     45      47       Karnataka  Andhra Pradesh   \n",
      "3  2023-03-01  2023-01-30     47      45  Andhra Pradesh       Karnataka   \n",
      "4  2023-03-01  2023-01-30     46       9      Tamil Nadu      Tamil Nadu   \n",
      "\n",
      "  srcid_tier destid_tier  cumsum_seatcount  cumsum_searchcount  dbd  \n",
      "0     Tier 1      Tier 1               8.0                76.0   30  \n",
      "1     Tier 1      Tier 1               8.0                70.0   30  \n",
      "2     Tier 1      Tier 1               4.0               142.0   30  \n",
      "3     Tier 1      Tier 1               0.0                68.0   30  \n",
      "4     Tier 1       Tier2               9.0               162.0   30  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import TimeSeriesSplit # For basic time-series split if full walk-forward is complex\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "import lightgbm as lgb # Or import xgboost as xgb\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Load datasets\n",
    "try:\n",
    "    train_df = pd.read_csv('train/train.csv')\n",
    "    test_df = pd.read_csv('test.csv')\n",
    "    # Assuming transactions.csv is now available\n",
    "    transactions_df = pd.read_csv('train/transactions.csv')\n",
    "    print(\"Data loaded successfully.\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Ensure 'train.csv', 'test.csv', and 'transactions.csv' are in the same directory.\")\n",
    "    exit()\n",
    "\n",
    "# Display initial information and sample rows\n",
    "print(\"\\n--- Train Data Info ---\")\n",
    "train_df.info()\n",
    "print(\"\\n--- Train Data Head ---\")\n",
    "print(train_df.head())\n",
    "\n",
    "print(\"\\n--- Test Data Info ---\")\n",
    "test_df.info()\n",
    "print(\"\\n--- Test Data Head ---\")\n",
    "print(test_df.head())\n",
    "\n",
    "print(\"\\n--- Transactions Data Info ---\")\n",
    "transactions_df.info()\n",
    "print(\"\\n--- Transactions Data Head ---\")\n",
    "print(transactions_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ac1b9c61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Processed Train Data Head ---\n",
      "           doj  srcid  destid  final_seatcount  year  month  day  day_of_week  \\\n",
      "18  2023-03-01     11      36           1288.0  2023      3    1            2   \n",
      "118 2023-03-02     11      36           1544.0  2023      3    2            3   \n",
      "218 2023-03-03     11      36           1744.0  2023      3    3            4   \n",
      "318 2023-03-04     11      36           1596.0  2023      3    4            5   \n",
      "418 2023-03-05     11      36           1100.0  2023      3    5            6   \n",
      "\n",
      "     day_of_year  week_of_year  ...  srcid_tier destid_region  destid_tier  \\\n",
      "18            60             9  ...      Tier 1         Delhi        Tier2   \n",
      "118           61             9  ...      Tier 1         Delhi        Tier2   \n",
      "218           62             9  ...      Tier 1         Delhi        Tier2   \n",
      "318           63             9  ...      Tier 1         Delhi        Tier2   \n",
      "418           64             9  ...      Tier 1         Delhi        Tier2   \n",
      "\n",
      "     route_mean_seatcount route_median_seatcount route_std_seatcount  \\\n",
      "18            2083.333333                 1944.0          913.886946   \n",
      "118           2083.333333                 1944.0          913.886946   \n",
      "218           2083.333333                 1944.0          913.886946   \n",
      "318           2083.333333                 1944.0          913.886946   \n",
      "418           2083.333333                 1944.0          913.886946   \n",
      "\n",
      "    final_seatcount_lag_7 final_seatcount_lag_14  rolling_mean_7_days  \\\n",
      "18            2083.333333            2083.333333          1288.000000   \n",
      "118           2083.333333            2083.333333          1416.000000   \n",
      "218           2083.333333            2083.333333          1525.333333   \n",
      "318           2083.333333            2083.333333          1543.000000   \n",
      "418           2083.333333            2083.333333          1454.400000   \n",
      "\n",
      "     rolling_std_7_days  \n",
      "18             0.000000  \n",
      "118          181.019336  \n",
      "218          228.572381  \n",
      "318          189.943851  \n",
      "418          257.504951  \n",
      "\n",
      "[5 rows x 25 columns]\n",
      "\n",
      "--- Processed Train Data NaNs ---\n",
      "doj                       0\n",
      "srcid                     0\n",
      "destid                    0\n",
      "final_seatcount           0\n",
      "year                      0\n",
      "month                     0\n",
      "day                       0\n",
      "day_of_week               0\n",
      "day_of_year               0\n",
      "week_of_year              0\n",
      "is_weekend                0\n",
      "route_id                  0\n",
      "avg_cumsum_seatcount      0\n",
      "avg_cumsum_searchcount    0\n",
      "srcid_region              0\n",
      "srcid_tier                0\n",
      "destid_region             0\n",
      "destid_tier               0\n",
      "route_mean_seatcount      0\n",
      "route_median_seatcount    0\n",
      "route_std_seatcount       0\n",
      "final_seatcount_lag_7     0\n",
      "final_seatcount_lag_14    0\n",
      "rolling_mean_7_days       0\n",
      "rolling_std_7_days        0\n",
      "dtype: int64\n",
      "\n",
      "--- Processed Test Data Head ---\n",
      "          route_key        doj  srcid  destid  year  month  day  day_of_week  \\\n",
      "0  2025-02-11_46_45 2025-02-11     46      45  2025      2   11            1   \n",
      "1  2025-01-20_17_23 2025-01-20     17      23  2025      1   20            0   \n",
      "2  2025-01-08_02_14 2025-01-08      2      14  2025      1    8            2   \n",
      "3  2025-01-08_08_47 2025-01-08      8      47  2025      1    8            2   \n",
      "4  2025-01-08_09_46 2025-01-08      9      46  2025      1    8            2   \n",
      "\n",
      "   day_of_year  week_of_year  ...  srcid_tier        destid_region  \\\n",
      "0           42             7  ...      Tier 1            Karnataka   \n",
      "1           20             4  ...       Tier2               East 1   \n",
      "2            8             2  ...      Tier 1  Maharashtra and Goa   \n",
      "3            8             2  ...       Tier2       Andhra Pradesh   \n",
      "4            8             2  ...       Tier2           Tamil Nadu   \n",
      "\n",
      "   destid_tier  route_mean_seatcount route_median_seatcount  \\\n",
      "0       Tier 1           4044.571429                 3836.0   \n",
      "1       Tier 1           1321.897321                 1285.0   \n",
      "2        Tier2           1538.325893                 1390.0   \n",
      "3       Tier 1           1481.488095                 1337.5   \n",
      "4       Tier 1           4814.683036                 4492.5   \n",
      "\n",
      "  route_std_seatcount final_seatcount_lag_7 final_seatcount_lag_14  \\\n",
      "0         1103.766561           4044.571429            4044.571429   \n",
      "1          389.919087           1321.897321            1321.897321   \n",
      "2          680.785677           1538.325893            1538.325893   \n",
      "3          616.389071           1481.488095            1481.488095   \n",
      "4         1586.108421           4814.683036            4814.683036   \n",
      "\n",
      "   rolling_mean_7_days  rolling_std_7_days  \n",
      "0          4044.571429         1103.766561  \n",
      "1          1321.897321          389.919087  \n",
      "2          1538.325893          680.785677  \n",
      "3          1481.488095          616.389071  \n",
      "4          4814.683036         1586.108421  \n",
      "\n",
      "[5 rows x 25 columns]\n",
      "\n",
      "--- Processed Test Data NaNs ---\n",
      "route_key                 0\n",
      "doj                       0\n",
      "srcid                     0\n",
      "destid                    0\n",
      "year                      0\n",
      "month                     0\n",
      "day                       0\n",
      "day_of_week               0\n",
      "day_of_year               0\n",
      "week_of_year              0\n",
      "is_weekend                0\n",
      "route_id                  0\n",
      "avg_cumsum_seatcount      0\n",
      "avg_cumsum_searchcount    0\n",
      "srcid_region              0\n",
      "srcid_tier                0\n",
      "destid_region             0\n",
      "destid_tier               0\n",
      "route_mean_seatcount      0\n",
      "route_median_seatcount    0\n",
      "route_std_seatcount       0\n",
      "final_seatcount_lag_7     0\n",
      "final_seatcount_lag_14    0\n",
      "rolling_mean_7_days       0\n",
      "rolling_std_7_days        0\n",
      "dtype: int64\n",
      "\n",
      "Features used for training: ['srcid', 'destid', 'year', 'month', 'day', 'day_of_week', 'day_of_year', 'week_of_year', 'is_weekend', 'avg_cumsum_seatcount', 'avg_cumsum_searchcount', 'srcid_region', 'srcid_tier', 'destid_region', 'destid_tier', 'route_mean_seatcount', 'route_median_seatcount', 'route_std_seatcount', 'final_seatcount_lag_7', 'final_seatcount_lag_14', 'rolling_mean_7_days', 'rolling_std_7_days']\n",
      "\n",
      "--- Training LightGBM Model ---\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000627 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2314\n",
      "[LightGBM] [Info] Number of data points in the train set: 48800, number of used features: 22\n",
      "[LightGBM] [Info] Start training from score 1630.000000\n",
      "\n",
      "Validation MAE: 178.75\n",
      "Validation RMSE: 276.18\n",
      "\n",
      "--- Training Final LightGBM Model on Full Data ---\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000542 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2314\n",
      "[LightGBM] [Info] Number of data points in the train set: 67200, number of used features: 22\n",
      "[LightGBM] [Info] Start training from score 1685.000000\n",
      "\n",
      "--- Generating Predictions for Test Set ---\n"
     ]
    }
   ],
   "source": [
    "def preprocess_and_feature_engineer(df, is_train=True, historical_data=None, transactions_data=None):\n",
    "    \"\"\"\n",
    "    Performs date preprocessing and feature engineering.\n",
    "    historical_data is used to calculate aggregates for the test set.\n",
    "    transactions_data is used to merge transaction-level features.\n",
    "    \"\"\"\n",
    "    df['doj'] = pd.to_datetime(df['doj'])\n",
    "\n",
    "    # Extract temporal features from doj\n",
    "    df['year'] = df['doj'].dt.year\n",
    "    df['month'] = df['doj'].dt.month\n",
    "    df['day'] = df['doj'].dt.day\n",
    "    df['day_of_week'] = df['doj'].dt.dayofweek\n",
    "    df['day_of_year'] = df['doj'].dt.dayofyear\n",
    "    df['week_of_year'] = df['doj'].dt.isocalendar().week.astype(int)\n",
    "    df['is_weekend'] = (df['doj'].dt.dayofweek >= 5).astype(int) # Saturday (5) or Sunday (6)\n",
    "\n",
    "    # Placeholder for public holidays (requires external data)\n",
    "    # For a real hackathon, this would involve loading a holiday calendar\n",
    "    # and creating features like 'is_holiday', 'days_until_holiday', etc.\n",
    "\n",
    "    # Combine srcid and destid to create a unique route identifier for aggregation\n",
    "    df['route_id'] = df['srcid'].astype(str) + '_' + df['destid'].astype(str)\n",
    "\n",
    "    # --- Integrate transactions data ---\n",
    "    if transactions_data is not None:\n",
    "        transactions_data['doj'] = pd.to_datetime(transactions_data['doj'])\n",
    "        transactions_data['doi'] = pd.to_datetime(transactions_data['doi'])\n",
    "\n",
    "        # Example: Aggregate transaction data to the same granularity as train/test_df\n",
    "        # This part needs to be carefully designed to avoid data leakage.\n",
    "        # For a 15-day prediction window, for a given doj in test_df, you can only use\n",
    "        # transaction data up to (doj - 15 days).\n",
    "        # This often involves a complex rolling join or pre-aggregation.\n",
    "        \n",
    "        # For now, let's assume we are creating features from transactions_data\n",
    "        # that are relevant for the 'doj' in the main dataframe.\n",
    "        # This is a placeholder and needs to be refined based on actual data structure and leakage prevention.\n",
    "        \n",
    "        # Example: Calculate average cumsum_seatcount and cumsum_searchcount for each route_id, doj\n",
    "        # This is a simplified aggregation. In reality, you'd need to consider the 'doi' and 'dbd'\n",
    "        # to ensure you're only using data available 15 days prior to 'doj'.\n",
    "        transactions_agg = transactions_data.groupby(['doj', 'srcid', 'destid']).agg(\n",
    "            avg_cumsum_seatcount=('cumsum_seatcount', 'mean'),\n",
    "            avg_cumsum_searchcount=('cumsum_searchcount', 'mean'),\n",
    "            # You might also want to capture the last known dbd, or average dbd\n",
    "            # avg_dbd=('dbd', 'mean')\n",
    "        ).reset_index()\n",
    "\n",
    "        df = pd.merge(df, transactions_agg, on=['doj', 'srcid', 'destid'], how='left')\n",
    "\n",
    "        # Also, directly use region and tier features from transactions_data\n",
    "        # Assuming srcid_region, destid_region, srcid_tier, destid_tier are consistent per srcid/destid\n",
    "        region_tier_map_src = transactions_data[['srcid', 'srcid_region', 'srcid_tier']].drop_duplicates().set_index('srcid')\n",
    "        region_tier_map_dest = transactions_data[['destid', 'destid_region', 'destid_tier']].drop_duplicates().set_index('destid')\n",
    "\n",
    "        df['srcid_region'] = df['srcid'].map(region_tier_map_src['srcid_region'])\n",
    "        df['srcid_tier'] = df['srcid'].map(region_tier_map_src['srcid_tier'])\n",
    "        df['destid_region'] = df['destid'].map(region_tier_map_dest['destid_region'])\n",
    "        df['destid_tier'] = df['destid'].map(region_tier_map_dest['destid_tier'])\n",
    "\n",
    "        # Fill any NaNs in new features (e.g., for routes not in transactions_data or future dates)\n",
    "        for col in ['avg_cumsum_seatcount', 'avg_cumsum_searchcount']:\n",
    "            df[col].fillna(0, inplace=True) # Or a more appropriate fill value like mean/median\n",
    "\n",
    "        # Convert new categorical features to category type\n",
    "        for col in ['srcid_region', 'destid_region', 'srcid_tier', 'destid_tier']:\n",
    "            df[col] = df[col].astype('category')\n",
    "\n",
    "    if is_train:\n",
    "        # Calculate route-specific aggregates from training data\n",
    "        route_aggregates = df.groupby('route_id')['final_seatcount'].agg(\n",
    "            route_mean_seatcount='mean',\n",
    "            route_median_seatcount='median',\n",
    "            route_std_seatcount='std'\n",
    "        ).reset_index()\n",
    "        df = pd.merge(df, route_aggregates, on='route_id', how='left')\n",
    "\n",
    "        # Create lagged features (for training data, fill NaNs appropriately)\n",
    "        df = df.sort_values(by=['route_id', 'doj'])\n",
    "        df['final_seatcount_lag_7'] = df.groupby('route_id')['final_seatcount'].shift(7)\n",
    "        df['final_seatcount_lag_14'] = df.groupby('route_id')['final_seatcount'].shift(14)\n",
    "\n",
    "        # Rolling statistics (for training data)\n",
    "        df['rolling_mean_7_days'] = df.groupby('route_id')['final_seatcount'].transform(\n",
    "            lambda x: x.rolling(window=7, min_periods=1).mean()\n",
    "        )\n",
    "        df['rolling_std_7_days'] = df.groupby('route_id')['final_seatcount'].transform(\n",
    "            lambda x: x.rolling(window=7, min_periods=1).std()\n",
    "        )\n",
    "\n",
    "        # Fill NaNs created by lagging/rolling (e.g., with route mean or 0)\n",
    "        df['final_seatcount_lag_7'].fillna(df['route_mean_seatcount'], inplace=True)\n",
    "        df['final_seatcount_lag_14'].fillna(df['route_mean_seatcount'], inplace=True)\n",
    "        df['rolling_mean_7_days'].fillna(df['route_mean_seatcount'], inplace=True)\n",
    "        df['rolling_std_7_days'].fillna(0, inplace=True)\n",
    "\n",
    "        return df, route_aggregates\n",
    "    else:\n",
    "        # For test data, use aggregates from the historical_data (training data)\n",
    "        df = pd.merge(df, historical_data, on='route_id', how='left')\n",
    "\n",
    "        # For test data, lagged features and rolling statistics need careful handling.\n",
    "        # This is a simplification; a more robust solution would involve iteratively\n",
    "        # predicting and then using those predictions to generate subsequent lags,\n",
    "        # or using features that are known 15 days in advance.\n",
    "        df['final_seatcount_lag_7'] = df['route_mean_seatcount'] # Placeholder\n",
    "        df['final_seatcount_lag_14'] = df['route_mean_seatcount'] # Placeholder\n",
    "        df['rolling_mean_7_days'] = df['route_mean_seatcount'] # Placeholder\n",
    "        df['rolling_std_7_days'] = df['route_std_seatcount'].fillna(0) # Placeholder\n",
    "\n",
    "        # Handle routes in test_df not present in train_df (if any)\n",
    "        # Corrected: Use the column names as they exist in historical_data\n",
    "        for col_name in ['route_mean_seatcount', 'route_median_seatcount', 'route_std_seatcount']:\n",
    "            df[col_name].fillna(historical_data[col_name].mean(), inplace=True)\n",
    "\n",
    "        return df\n",
    "\n",
    "# Process training data\n",
    "train_df_processed, route_aggregates_for_test = preprocess_and_feature_engineer(train_df.copy(), is_train=True, transactions_data=transactions_df.copy())\n",
    "\n",
    "# Process test data\n",
    "# For test data, ensure transactions_data is appropriately filtered to avoid data leakage.\n",
    "# For a 15-day prediction window, for a test_doj, only use transactions where doi <= (test_doj - 15 days).\n",
    "# This is a complex step and simplified here.\n",
    "test_df_processed = preprocess_and_feature_engineer(test_df.copy(), is_train=False, historical_data=route_aggregates_for_test, transactions_data=transactions_df.copy())\n",
    "\n",
    "# Display processed data head and check for NaNs\n",
    "print(\"\\n--- Processed Train Data Head ---\")\n",
    "print(train_df_processed.head())\n",
    "print(\"\\n--- Processed Train Data NaNs ---\")\n",
    "print(train_df_processed.isnull().sum())\n",
    "\n",
    "print(\"\\n--- Processed Test Data Head ---\")\n",
    "print(test_df_processed.head())\n",
    "print(\"\\n--- Processed Test Data NaNs ---\")\n",
    "print(test_df_processed.isnull().sum())\n",
    "\n",
    "# Define features and target\n",
    "features = [col for col in train_df_processed.columns if col not in ['doj', 'final_seatcount', 'route_key', 'route_id']]\n",
    "target = 'final_seatcount'\n",
    "\n",
    "# Convert categorical features for LightGBM\n",
    "categorical_features = ['srcid', 'destid', 'month', 'day_of_week', 'is_weekend',\n",
    "                        'srcid_region', 'destid_region', 'srcid_tier', 'destid_tier'] # Added new categorical features\n",
    "for col in categorical_features:\n",
    "    if col in features: # Check if feature exists after processing\n",
    "        train_df_processed[col] = train_df_processed[col].astype('category')\n",
    "        test_df_processed[col] = test_df_processed[col].astype('category')\n",
    "\n",
    "print(f\"\\nFeatures used for training: {features}\")\n",
    "\n",
    "# --- Model Training (Simplified Train/Validation Split for demonstration) ---\n",
    "# For a real hackathon, implement full Walk-Forward Validation as discussed.\n",
    "# Here, we'll simulate a simple time-based split for quick demonstration.\n",
    "# Sort data by date for time-series split\n",
    "train_df_processed = train_df_processed.sort_values(by='doj').reset_index(drop=True)\n",
    "\n",
    "# Define a cutoff date for validation\n",
    "train_cutoff_date = pd.to_datetime('2024-07-01') # Example: Use data before July 2024 for training\n",
    "\n",
    "X_train_val = train_df_processed[train_df_processed['doj'] < train_cutoff_date][features]\n",
    "y_train_val = train_df_processed[train_df_processed['doj'] < train_cutoff_date][target]\n",
    "X_val = train_df_processed[train_df_processed['doj'] >= train_cutoff_date][features]\n",
    "y_val = train_df_processed[train_df_processed['doj'] >= train_cutoff_date][target]\n",
    "\n",
    "# Initialize LightGBM Regressor Model\n",
    "lgb_model = lgb.LGBMRegressor(objective='regression_l1', # MAE objective\n",
    "                              metric='mae',\n",
    "                              n_estimators=1000,\n",
    "                              learning_rate=0.05,\n",
    "                              num_leaves=31,\n",
    "                              max_depth=-1,\n",
    "                              min_child_samples=20,\n",
    "                              subsample=0.8,\n",
    "                              colsample_bytree=0.8,\n",
    "                              random_state=42,\n",
    "                              n_jobs=-1)\n",
    "\n",
    "print(\"\\n--- Training LightGBM Model ---\")\n",
    "lgb_model.fit(X_train_val, y_train_val,\n",
    "              eval_set=[(X_val, y_val)],\n",
    "              eval_metric='mae',\n",
    "              callbacks=[lgb.early_stopping(100, verbose=False)], # Early stopping to prevent overfitting\n",
    "              categorical_feature=[col for col in categorical_features if col in features])\n",
    "\n",
    "# Evaluate on validation set\n",
    "val_predictions = lgb_model.predict(X_val)\n",
    "val_mae = mean_absolute_error(y_val, val_predictions)\n",
    "val_rmse = np.sqrt(mean_squared_error(y_val, val_predictions))\n",
    "print(f\"\\nValidation MAE: {val_mae:.2f}\")\n",
    "print(f\"Validation RMSE: {val_rmse:.2f}\")\n",
    "\n",
    "# --- Final Model Training on Full Training Data ---\n",
    "print(\"\\n--- Training Final LightGBM Model on Full Data ---\")\n",
    "final_lgb_model = lgb.LGBMRegressor(objective='regression_l1', # MAE objective\n",
    "                                    metric='mae',\n",
    "                                    n_estimators=lgb_model.best_iteration_, # Use best iteration from validation\n",
    "                                    learning_rate=0.05,\n",
    "                                    num_leaves=31,\n",
    "                                    max_depth=-1,\n",
    "                                    min_child_samples=20,\n",
    "                                    subsample=0.8,\n",
    "                                    colsample_bytree=0.8,\n",
    "                                    random_state=42,\n",
    "                                    n_jobs=-1)\n",
    "\n",
    "final_lgb_model.fit(train_df_processed[features], train_df_processed[target],\n",
    "                    categorical_feature=[col for col in categorical_features if col in features])\n",
    "\n",
    "# --- Generate Predictions for Test Set ---\n",
    "print(\"\\n--- Generating Predictions for Test Set ---\")\n",
    "test_predictions = final_lgb_model.predict(test_df_processed[features])\n",
    "\n",
    "# Ensure predictions are non-negative and round to nearest integer if required\n",
    "test_predictions[test_predictions < 0] = 0\n",
    "test_predictions = np.round(test_predictions).astype(int) # Assuming integer seat counts"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
